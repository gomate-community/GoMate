#!/usr/bin/env python
# -*- coding:utf-8 _*-
"""
@author:quincy qiang
@license: Apache Licence
@file: app.py
@time: 2024/05/21
@contact: yanqiangmiffy@gamil.com
"""
import os
import shutil

import gradio as gr

from gomate.applications.rag_weibo import WeiboRagApplication, ApplicationConfig
from gomate.modules.document.utils import PROJECT_BASE
from gomate.modules.reranker.bge_reranker import BgeRerankerConfig
from gomate.modules.retrieval.dense_retriever import DenseRetrieverConfig

# ä¿®æ”¹æˆè‡ªå·±çš„é…ç½®ï¼ï¼ï¼
app_config = ApplicationConfig()
app_config.llm_model_path = "H:/pretrained_models/llm/glm-4-9b-chat"
app_config.docs_path = 'H:/Projects/Weibo Insight/data/docs/weibo'
retriever_config = DenseRetrieverConfig(
    model_name_or_path="H:/pretrained_models/mteb/bge-large-zh-v1.5",
    dim=1024,
    index_path=os.path.join(PROJECT_BASE, 'output/weibo_dense')
)
rerank_config = BgeRerankerConfig(
    model_name_or_path="H:/pretrained_models/mteb/bge-reranker-large"
)

app_config.retriever_config = retriever_config
app_config.rerank_config = rerank_config
application = WeiboRagApplication(app_config)


# application.init_vector_store()


def get_file_list():
    if not os.path.exists(app_config.docs_path):
        return []
    return [f for f in os.listdir(app_config.docs_path)]


file_list = get_file_list()


def info_fn(filename):
    gr.Info(f"upload file:{filename} success!")


def upload_file(file):
    cache_base_dir = app_config.docs_path
    if not os.path.exists(cache_base_dir):
        os.mkdir(cache_base_dir)
    filename = os.path.basename(file.name)
    shutil.move(file.name, cache_base_dir + filename)
    # file_listé¦–ä½æ’å…¥æ–°ä¸Šä¼ çš„æ–‡ä»¶
    file_list.insert(0, filename)
    application.add_document(app_config.docs_path + filename)
    info_fn(filename)
    return gr.Dropdown(choices=file_list, value=filename, interactive=True)


def set_knowledge(kg_name, history):
    try:
        application.load_vector_store()
        msg_status = f'{kg_name}çŸ¥è¯†åº“å·²æˆåŠŸåŠ è½½'
    except Exception as e:
        print(e)
        msg_status = f'{kg_name}çŸ¥è¯†åº“æœªæˆåŠŸåŠ è½½'
    return history + [[None, msg_status]]


def clear_session():
    return '', None


def predict(input,
            large_language_model,
            embedding_model,
            top_k,
            use_web,
            use_pattern,
            history=None):
    # print(large_language_model, embedding_model)
    print(input)
    if history == None:
        history = []

    if use_web == 'ä½¿ç”¨':
        web_content = application.retriever.search_web(query=input)
    else:
        web_content = ''
    search_text = ''
    if use_pattern == 'æ¨¡å‹é—®ç­”':
        result = application.get_llm_answer(query=input, web_content=web_content)
        history.append((input, result))
        search_text += web_content
        return '', history, history, search_text

    else:
        response, _, contents = application.chat(
            question=input,
            top_k=top_k,
        )
        history.append((input, response))
        for idx, source in enumerate(contents[:5]):
            sep = f'----------ã€æœç´¢ç»“æœ{idx + 1}ï¼šã€‘---------------\n'
            search_text += f'{sep}\n{source}\n\n'
        # print(search_text)
        search_text += "----------ã€ç½‘ç»œæ£€ç´¢å†…å®¹ã€‘-----------\n"
        search_text += web_content
        print("--------------------ã€æ¨¡å‹å›ç­”ã€‘----------------\n")
        print(response)
        return '', history, history, search_text


with gr.Blocks(theme="soft") as demo:
    gr.Markdown("""<h1><center>äº¤äº’å¼å¾®åšèˆ†æƒ…é—®ç­”åŠ©æ‰‹</center></h1>
        <center><font size=3>
        </center></font>
        """)
    state = gr.State()

    with gr.Row():
        with gr.Column(scale=1):
            embedding_model = gr.Dropdown([
                "text2vec-base",
                "bge-large-v1.5",
                "bge-base-v1.5",
            ],
                label="Embedding model",
                value="bge-large-v1.5")

            large_language_model = gr.Dropdown(
                [
                    "ChatGLM3-9B",
                ],
                label="large language model",
                value="ChatGLM3-9B")

            top_k = gr.Slider(1,
                              20,
                              value=4,
                              step=1,
                              label="æ£€ç´¢top-kæ–‡æ¡£",
                              interactive=True)

            use_web = gr.Radio(["ä½¿ç”¨", "ä¸ä½¿ç”¨"], label="web search",
                               info="æ˜¯å¦ä½¿ç”¨ç½‘ç»œæœç´¢ï¼Œä½¿ç”¨æ—¶ç¡®ä¿ç½‘ç»œé€šå¸¸",
                               value="ä¸ä½¿ç”¨", interactive=False
                               )
            use_pattern = gr.Radio(
                [
                    'æ¨¡å‹é—®ç­”',
                    'çŸ¥è¯†åº“é—®ç­”',
                ],
                label="æ¨¡å¼",
                value='çŸ¥è¯†åº“é—®ç­”',
                interactive=False)

            kg_name = gr.Radio(["å¾®åšçŸ¥è¯†åº“"],
                               label="çŸ¥è¯†åº“",
                               value=None,
                               info="ä½¿ç”¨çŸ¥è¯†åº“é—®ç­”ï¼Œè¯·åŠ è½½çŸ¥è¯†åº“",
                               interactive=True)
            set_kg_btn = gr.Button("åŠ è½½çŸ¥è¯†åº“")

            file = gr.File(label="å°†æ–‡ä»¶ä¸Šä¼ åˆ°çŸ¥è¯†åº“åº“ï¼Œå†…å®¹è¦å°½é‡åŒ¹é…",
                           visible=True,
                           file_types=['.txt', '.md', '.docx', '.pdf']
                           )
            # uploaded_files = gr.Dropdown(
            #     file_list,
            #     label="å·²ä¸Šä¼ çš„æ–‡ä»¶åˆ—è¡¨",
            #     value=file_list[0] if len(file_list) > 0 else '',
            #     interactive=True
            # )
        with gr.Column(scale=4):
            with gr.Row():
                chatbot = gr.Chatbot(label='Weibo Insight Application',height=650)
            with gr.Row():
                message = gr.Textbox(label='è¯·è¾“å…¥é—®é¢˜')
            with gr.Row():
                clear_history = gr.Button("ğŸ§¹ æ¸…é™¤å†å²å¯¹è¯")
                send = gr.Button("ğŸš€ å‘é€")
            with gr.Row():
                gr.Markdown("""æé†’ï¼š<br>
                                        [Weibo Insight](https://github.com/Weibo-Insight/WeiboInsight) <br>
                                        æœ‰ä»»ä½•ä½¿ç”¨é—®é¢˜[Github IssueåŒº](https://github.com/Weibo-Insight/WeiboInsight)è¿›è¡Œåé¦ˆ. 
                                        <br>
                                        """)
        with gr.Column(scale=2):
            search = gr.Textbox(label='æœç´¢ç»“æœ')

        # ============= è§¦å‘åŠ¨ä½œ=============
        file.upload(upload_file,
                    inputs=file,
                    outputs=None)
        set_kg_btn.click(
            set_knowledge,
            show_progress="full",
            inputs=[kg_name, chatbot],
            outputs=chatbot
        )
        # å‘é€æŒ‰é’® æäº¤
        send.click(predict,
                   inputs=[
                       message,
                       large_language_model,
                       embedding_model,
                       top_k,
                       use_web,
                       use_pattern,
                       state
                   ],
                   outputs=[message, chatbot, state, search])

        # æ¸…ç©ºå†å²å¯¹è¯æŒ‰é’® æäº¤
        clear_history.click(fn=clear_session,
                            inputs=[],
                            outputs=[chatbot, state],
                            queue=False)

        # è¾“å…¥æ¡† å›è½¦
        message.submit(predict,
                       inputs=[
                           message,
                           large_language_model,
                           embedding_model,
                           top_k,
                           use_web,
                           use_pattern,
                           state
                       ],
                       outputs=[message, chatbot, state, search])

demo.queue().launch(
    server_name='0.0.0.0',
    server_port=7860,
    share=True,
    show_error=True,
    debug=True,
    # enable_queue=True,
    inbrowser=False,
)
